<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>📈 Theory overview &mdash; PUNCC 0.8.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="🖼️ Plotting" href="plotting.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            PUNCC
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">🚀 Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#conformal-regression">📈 Conformal Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#diabetes-dataset">💾 Diabetes Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#prediction-model">🔮 Prediction model</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#conformal-prediction">⚙️ Conformal prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#conformal-classification">📊 Conformal Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#mnist-dataset">💾 MNIST Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#id1">🔮 Prediction Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#id2">⚙️ Conformal prediction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#conformal-anomaly-detection">🚩 Conformal Anomaly Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#two-moons-dataset">💾 Two moons Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#anomaly-detection-model">🔮 Anomaly detection model</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#id3">⚙️ Conformal Anomaly Detection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="regression.html">📈 Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.SplitCP"><code class="docutils literal notranslate"><span class="pre">SplitCP</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.SplitCP.fit"><code class="docutils literal notranslate"><span class="pre">SplitCP.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.SplitCP.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">SplitCP.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.SplitCP.predict"><code class="docutils literal notranslate"><span class="pre">SplitCP.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.LocallyAdaptiveCP"><code class="docutils literal notranslate"><span class="pre">LocallyAdaptiveCP</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.LocallyAdaptiveCP.fit"><code class="docutils literal notranslate"><span class="pre">LocallyAdaptiveCP.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.LocallyAdaptiveCP.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">LocallyAdaptiveCP.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.LocallyAdaptiveCP.predict"><code class="docutils literal notranslate"><span class="pre">LocallyAdaptiveCP.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.CQR"><code class="docutils literal notranslate"><span class="pre">CQR</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CQR.fit"><code class="docutils literal notranslate"><span class="pre">CQR.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CQR.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">CQR.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CQR.predict"><code class="docutils literal notranslate"><span class="pre">CQR.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.CVPlus"><code class="docutils literal notranslate"><span class="pre">CVPlus</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CVPlus.fit"><code class="docutils literal notranslate"><span class="pre">CVPlus.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CVPlus.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">CVPlus.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.CVPlus.predict"><code class="docutils literal notranslate"><span class="pre">CVPlus.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.EnbPI"><code class="docutils literal notranslate"><span class="pre">EnbPI</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.EnbPI.fit"><code class="docutils literal notranslate"><span class="pre">EnbPI.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.EnbPI.predict"><code class="docutils literal notranslate"><span class="pre">EnbPI.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="regression.html#deel.puncc.regression.AdaptiveEnbPI"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnbPI</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.AdaptiveEnbPI.fit"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnbPI.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="regression.html#deel.puncc.regression.AdaptiveEnbPI.predict"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnbPI.predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">📊 Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="classification.html#deel.puncc.classification.LAC"><code class="docutils literal notranslate"><span class="pre">LAC</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.LAC.fit"><code class="docutils literal notranslate"><span class="pre">LAC.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.LAC.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">LAC.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.LAC.predict"><code class="docutils literal notranslate"><span class="pre">LAC.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="classification.html#deel.puncc.classification.RAPS"><code class="docutils literal notranslate"><span class="pre">RAPS</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.RAPS.fit"><code class="docutils literal notranslate"><span class="pre">RAPS.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.RAPS.predict"><code class="docutils literal notranslate"><span class="pre">RAPS.predict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="classification.html#deel.puncc.classification.APS"><code class="docutils literal notranslate"><span class="pre">APS</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.APS.fit"><code class="docutils literal notranslate"><span class="pre">APS.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="classification.html#deel.puncc.classification.APS.predict"><code class="docutils literal notranslate"><span class="pre">APS.predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="object_detection.html">🖼️ Object Detection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="object_detection.html#deel.puncc.object_detection.SplitBoxWise"><code class="docutils literal notranslate"><span class="pre">SplitBoxWise</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="object_detection.html#deel.puncc.object_detection.SplitBoxWise.fit"><code class="docutils literal notranslate"><span class="pre">SplitBoxWise.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="object_detection.html#deel.puncc.object_detection.SplitBoxWise.get_nonconformity_scores"><code class="docutils literal notranslate"><span class="pre">SplitBoxWise.get_nonconformity_scores()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="object_detection.html#deel.puncc.object_detection.SplitBoxWise.predict"><code class="docutils literal notranslate"><span class="pre">SplitBoxWise.predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="anomaly_detection.html">🚩 Anomaly detection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="anomaly_detection.html#deel.puncc.anomaly_detection.SplitCAD"><code class="docutils literal notranslate"><span class="pre">SplitCAD</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="anomaly_detection.html#deel.puncc.anomaly_detection.SplitCAD.fit"><code class="docutils literal notranslate"><span class="pre">SplitCAD.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="anomaly_detection.html#deel.puncc.anomaly_detection.SplitCAD.predict"><code class="docutils literal notranslate"><span class="pre">SplitCAD.predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">💻 API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api.html#api-s-modules">API’s Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="conformalization.html">Conformalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="conformalization.html#conformalization.ConformalPredictor"><code class="docutils literal notranslate"><span class="pre">ConformalPredictor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="conformalization.html#conformalization.CrossValCpAggregator"><code class="docutils literal notranslate"><span class="pre">CrossValCpAggregator</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="prediction.html">Prediction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="prediction.html#prediction.BasePredictor"><code class="docutils literal notranslate"><span class="pre">BasePredictor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction.html#prediction.DualPredictor"><code class="docutils literal notranslate"><span class="pre">DualPredictor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction.html#prediction.IdPredictor"><code class="docutils literal notranslate"><span class="pre">IdPredictor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction.html#prediction.MeanVarPredictor"><code class="docutils literal notranslate"><span class="pre">MeanVarPredictor</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="experimental.html">Experimental</a><ul>
<li class="toctree-l4"><a class="reference internal" href="experimental.html#experimental.TorchPredictor"><code class="docutils literal notranslate"><span class="pre">TorchPredictor</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="calibration.html">Calibration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="calibration.html#calibration.BaseCalibrator"><code class="docutils literal notranslate"><span class="pre">BaseCalibrator</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="calibration.html#calibration.CvPlusCalibrator"><code class="docutils literal notranslate"><span class="pre">CvPlusCalibrator</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="calibration.html#calibration.ScoreCalibrator"><code class="docutils literal notranslate"><span class="pre">ScoreCalibrator</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="splitting.html">Splitting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="splitting.html#splitting.BaseSplitter"><code class="docutils literal notranslate"><span class="pre">BaseSplitter</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="splitting.html#splitting.IdSplitter"><code class="docutils literal notranslate"><span class="pre">IdSplitter</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="splitting.html#splitting.RandomSplitter"><code class="docutils literal notranslate"><span class="pre">RandomSplitter</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="splitting.html#splitting.KFoldSplitter"><code class="docutils literal notranslate"><span class="pre">KFoldSplitter</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="utils.html">Utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="utils.html#deel.puncc.api.utils.alpha_calib_check"><code class="docutils literal notranslate"><span class="pre">alpha_calib_check()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="utils.html#deel.puncc.api.utils.quantile"><code class="docutils literal notranslate"><span class="pre">quantile()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nonconformity_scores.html">Nonconformity scores</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.absolute_difference"><code class="docutils literal notranslate"><span class="pre">absolute_difference()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.cqr_score"><code class="docutils literal notranslate"><span class="pre">cqr_score()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.difference"><code class="docutils literal notranslate"><span class="pre">difference()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.lac_score"><code class="docutils literal notranslate"><span class="pre">lac_score()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.raps_score"><code class="docutils literal notranslate"><span class="pre">raps_score()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.raps_score_builder"><code class="docutils literal notranslate"><span class="pre">raps_score_builder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.scaled_ad"><code class="docutils literal notranslate"><span class="pre">scaled_ad()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="nonconformity_scores.html#nonconformity_scores.scaled_bbox_difference"><code class="docutils literal notranslate"><span class="pre">scaled_bbox_difference()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="prediction_sets.html">Prediction sets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.constant_bbox"><code class="docutils literal notranslate"><span class="pre">constant_bbox()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.constant_interval"><code class="docutils literal notranslate"><span class="pre">constant_interval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.cqr_interval"><code class="docutils literal notranslate"><span class="pre">cqr_interval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.lac_set"><code class="docutils literal notranslate"><span class="pre">lac_set()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.raps_set"><code class="docutils literal notranslate"><span class="pre">raps_set()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.raps_set_builder"><code class="docutils literal notranslate"><span class="pre">raps_set_builder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.scaled_bbox"><code class="docutils literal notranslate"><span class="pre">scaled_bbox()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="prediction_sets.html#prediction_sets.scaled_interval"><code class="docutils literal notranslate"><span class="pre">scaled_interval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api.html#conformalpredictor">ConformalPredictor</a></li>
<li class="toctree-l3"><a class="reference internal" href="api.html#predictor">Predictor</a></li>
<li class="toctree-l3"><a class="reference internal" href="api.html#calibrator">Calibrator</a></li>
<li class="toctree-l3"><a class="reference internal" href="api.html#splitter">Splitter</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">📏 Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.classification_mean_coverage"><code class="docutils literal notranslate"><span class="pre">classification_mean_coverage()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.classification_mean_size"><code class="docutils literal notranslate"><span class="pre">classification_mean_size()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.iou"><code class="docutils literal notranslate"><span class="pre">iou()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.object_detection_mean_area"><code class="docutils literal notranslate"><span class="pre">object_detection_mean_area()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.object_detection_mean_coverage"><code class="docutils literal notranslate"><span class="pre">object_detection_mean_coverage()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.regression_ace"><code class="docutils literal notranslate"><span class="pre">regression_ace()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.regression_mean_coverage"><code class="docutils literal notranslate"><span class="pre">regression_mean_coverage()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#metrics.regression_sharpness"><code class="docutils literal notranslate"><span class="pre">regression_sharpness()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="plotting.html">🖼️ Plotting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="plotting.html#plotting.draw_bounding_box"><code class="docutils literal notranslate"><span class="pre">draw_bounding_box()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plotting.html#plotting.plot_prediction_intervals"><code class="docutils literal notranslate"><span class="pre">plot_prediction_intervals()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">📈 Theory overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#uncertainty-quantification">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conformal-prediction">Conformal Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conformal-regression">Conformal Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#split-inductive-conformal">Split (inductive) Conformal</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#locally-adaptive-conformal-regression">Locally Adaptive Conformal Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conformalized-quantile-regression-cqr">Conformalized Quantile Regression (CQR)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#cross-validation-cv-jackknife">Cross-validation+ (CV+), Jackknife+</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ensemble-batch-prediction-intervals-enbpi">Ensemble Batch Prediction Intervals (EnbPI)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conformal-classification">Conformal Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#least-ambiguous-set-valued-classifiers-lac">Least Ambiguous Set-Valued Classifiers (LAC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-prediction-sets-aps">Adaptive Prediction Sets (APS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#regularized-adaptive-prediction-sets-raps">Regularized Adaptive Prediction Sets (RAPS)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conformal-anomaly-detection">Conformal Anomaly Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conformal-object-detection">Conformal Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PUNCC</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">📈 Theory overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/theory_overview.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="theory-overview">
<span id="id1"></span><h1>📈 Theory overview<a class="headerlink" href="#theory-overview" title="Permalink to this heading"></a></h1>
<section id="uncertainty-quantification">
<h2>Uncertainty Quantification<a class="headerlink" href="#uncertainty-quantification" title="Permalink to this heading"></a></h2>
<p>In machine learning, we build predictive models from experience,
by choosing the right approach for the right problem, and from the accessible
data, via algorithms. Despite our best efforts, we can encounter some
underlying uncertainty that could stem from various sources or causes.</p>
<dl class="simple">
<dt>Typically, uncertainty in the machine learning process can be categorized into two types:</dt><dd><ul class="simple">
<li><p>Aleatoric uncertainty, also known as statistical uncertainty, which is <em>irreducible</em> as due to the intrinsic randomness of the phenomenon being modeled</p></li>
<li><p>Epistemic uncertainty, also known as systematic uncertainty, which is <em>reducible</em> through additional information, e.g. via more data or better models</p></li>
</ul>
</dd>
</dl>
<p>Depending on the application fields of machine learning models, uncertainty can have major impacts on performance and/or safety.</p>
</section>
<section id="conformal-prediction">
<h2>Conformal Prediction<a class="headerlink" href="#conformal-prediction" title="Permalink to this heading"></a></h2>
<p>Conformal Prediction (CP) is a set of methods to estimate uncertainty
by constructing by constructing <strong>valid</strong> <em>prediction sets</em>,
i.e. prediction sets with a probabilistic guarantee
of marginal coverage.
The following three features make CP methods particularly attractive:
- <em>Distribution-free</em>. CP methods can be applied regardless of the underlying data-generating distribution.
- <em>Model-agnostic</em>. CP works with any ML model, even with black-box models where we only have access to the outputs of the model.
- <em>Non-asymptotic</em>. CP methods provide finite-sample probabilistic guarantees, that is, the guarantees hold without the need to assume that the number of available data grows to infinity.</p>
<p>Given an error rate (or significance level) <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span>, set by the user, a set of exchangeable (or more simply i.i.d.)
train data <span class="math notranslate nohighlight">\(\{ (X_i, Y_i) \}_{i=1}^{n}\)</span> and a test point
<span class="math notranslate nohighlight">\((X_{new}, Y_{new})\)</span>,
all of which are generated from the same joint distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{XY}\)</span>,
a conformal prediction procedure uses the training data
to build prediction sets <span class="math notranslate nohighlight">\(\widehat{C}_{\alpha}(\cdot)\)</span> so that:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P} \Big\{ Y_{new} \in \widehat{C}_{\alpha}\left(X_{new}\right) \Big\} \geq 1 - \alpha.\]</div>
<p>Over many calibration and test sets, <span class="math notranslate nohighlight">\(\widehat{C}_{\alpha}(X_{new})\)</span> will contain
the observed values of <span class="math notranslate nohighlight">\(Y_{new}\)</span> with frequency of <em>at least</em> <span class="math notranslate nohighlight">\((1-\alpha)\)</span>.</p>
<p>Usually, the conformal prediction method uses a point-predictor model <span class="math notranslate nohighlight">\(\widehat{f}\)</span>
and turns it into the set predictor <span class="math notranslate nohighlight">\(C_\alpha\)</span>
via a calibration procedure.
Within the conformal prediction framework,
the inequality above holds for any model,
any data distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{XY}\)</span> and any training set sample size, under the following minimal assumptions:
- <em>Exchangeability</em>. The data <span class="math notranslate nohighlight">\((X_1,Y_i),\dots, (X_n, Y_n), (X_{new}, Y_{new})\)</span> form an exchangeable sequence (this is a milder assumption than the data being i.i.d.).
- <em>Independence of train and calibration data.</em> The data for the model training is independent from the data for the model calibration.</p>
<p>It is noteworthy that the coverage probability is marginalized over <span class="math notranslate nohighlight">\(X\)</span>.
Therefore, the CP algorithm is likely to achieve the coverage rate of <span class="math notranslate nohighlight">\(1-\alpha\)</span>
by under-covering conditionally to some specific regions in the space of <span class="math notranslate nohighlight">\(X\)</span> and over-covering in other regions.</p>
<p>Conformal prediction can act as a <em>post-processing procedure</em> to attain rigorous probability coverages,
as it can “conformalize” any existing predictor during or after training (black box predictors),
yielding marginally valid prediction sets.</p>
<p>In this page, we present the most common conformal prediction methods of the
literature used on regression and classification models. We also refer to
Angelopoulos and Bates <a class="reference internal" href="#angelopoulos2022" id="id2"><span>[Angelopoulos2022]</span></a> for a hands-on introduction to conformal prediction
and awesome conformal prediction <a class="reference external" href="https://github.com/valeman/awesome-conformal-prediction">github</a> for additional ressources.</p>
<p>In the following, let <span class="math notranslate nohighlight">\(D = {(X_i, Y_i)}_{i=1}^n \sim P_{XY}\)</span>
be the training data and <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span> the significance level (target maximum error rate).</p>
</section>
<section id="conformal-regression">
<h2>Conformal Regression<a class="headerlink" href="#conformal-regression" title="Permalink to this heading"></a></h2>
<section id="split-inductive-conformal">
<h3>Split (inductive) Conformal<a class="headerlink" href="#split-inductive-conformal" title="Permalink to this heading"></a></h3>
<p id="theory-splitcp">The split (also called inductive) conformal prediction <a class="reference internal" href="#papadopoulos2002" id="id3"><span>[Papadopoulos2002]</span></a> <a class="reference internal" href="#lei2018" id="id4"><span>[Lei2018]</span></a> requires a hold-out calibration
dataset: the dataset <span class="math notranslate nohighlight">\(D\)</span> is split into a proper training set
<span class="math notranslate nohighlight">\(D_{train}=\big\lbrace(X_i,Y_i), i=1,\dots,n_{train}\big\rbrace\)</span>
and an independent calibration dataset <span class="math notranslate nohighlight">\(D_{calib}=\big\lbrace(X_i,Y_i),i=1,\dots,n_{calib}\big\rbrace\)</span>.
The purpose of the calibration dataset is
to estimate prediction errors and use them to build the prediction interval for a new sample <span class="math notranslate nohighlight">\(X_{new}\)</span>.</p>
<p>Given a prediction model <span class="math notranslate nohighlight">\(\widehat{f}\)</span> trained on <span class="math notranslate nohighlight">\(D_{train}\)</span>, the algorithm is summarized in the following:</p>
<ol class="arabic simple">
<li><p>Choose a nonconformity score <span class="math notranslate nohighlight">\(s\)</span>, and define the error <span class="math notranslate nohighlight">\(R\)</span> over a sample <span class="math notranslate nohighlight">\((X,Y)\)</span> as <span class="math notranslate nohighlight">\(R = s(\widehat{f}(X),Y)\)</span>. For example, one can pick the absolute deviation <span class="math notranslate nohighlight">\(R = |\widehat{f}(X)-Y|\)</span>.</p></li>
<li><p>Compute the nonconformity scores on the calibration dataset: <span class="math notranslate nohighlight">\(\mathcal{R} = \{R_i\}_{}\)</span>, where <span class="math notranslate nohighlight">\(R_i=s(\widehat{f}(X_i), Y_i)\)</span> for <span class="math notranslate nohighlight">\(i=1,\dots,n_{calib}\)</span>.</p></li>
<li><p>Compute the error margin <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)(1 + 1/n_{calib})\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
<li><p>Build the prediction interval as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new}) = \Big[ \widehat{f}(X_{new}) - \delta_{\alpha} \,,\, \widehat{f}(X_{new}) + \delta_{\alpha} \Big].\]</div>
<p>Note that this procedure yields a constant-width prediction interval centered on the point estimate <span class="math notranslate nohighlight">\(\widehat{f}(X_{new})\)</span>.</p>
<p>In the literature, the split conformal procedure has been combined with different nonconformity scores,
which produced several methods. Some of them are presented hereafter.</p>
<section id="locally-adaptive-conformal-regression">
<h4>Locally Adaptive Conformal Regression<a class="headerlink" href="#locally-adaptive-conformal-regression" title="Permalink to this heading"></a></h4>
<p id="theory-lacp">The locally adaptive conformal regression <a class="reference internal" href="#papadopoulos2008" id="id5"><span>[Papadopoulos2008]</span></a> relies on scaled nonconformity scores:</p>
<div class="math notranslate nohighlight">
\[R_i = \frac{|\widehat{f}(X_i) - Y_i|}{\widehat{\sigma}(X_i)},\]</div>
<p>where <span class="math notranslate nohighlight">\(\widehat{\sigma}(X_i)\)</span> is a measure of dispersion of the nonconformity scores at <span class="math notranslate nohighlight">\(X_i\)</span>.
Usually, <span class="math notranslate nohighlight">\(\widehat{\sigma}\)</span> is trained to estimate the absolute prediction
error <span class="math notranslate nohighlight">\(|\widehat{f}(X)-Y|\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span>. The prediction interval is again
centered on <span class="math notranslate nohighlight">\(\widehat{f}(X_{new})\)</span> but the margins are scaled w.r.t the estimated local variability at <span class="math notranslate nohighlight">\(Y | X = X_{new}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new})=
\Big[ \widehat{f}(X_{new}) - \widehat{\sigma}(X_{new})\, \delta_{\alpha} \,,\, \widehat{f}(X_{new}) + \widehat{\sigma}(X_{new}) \, \delta_{\alpha} \Big].\]</div>
<p>The prediction intervals are therefore of variable width, which is more adaptive to heteroskedascity and
usually improve the conditional coverage. The price is the higher computational cost due to fitting two functions
<span class="math notranslate nohighlight">\(\widehat{f}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\sigma}\)</span>, on the proper training set.</p>
</section>
<section id="conformalized-quantile-regression-cqr">
<h4>Conformalized Quantile Regression (CQR)<a class="headerlink" href="#conformalized-quantile-regression-cqr" title="Permalink to this heading"></a></h4>
<p id="theory-cqr">Split conformal prediction can be extended to <a class="reference external" href="https://en.wikipedia.org/wiki/Quantile_regression">quantile predictors</a>  <span class="math notranslate nohighlight">\(q(\cdot)\)</span>.
Given a nominal error rate <span class="math notranslate nohighlight">\(\alpha,\)</span>
and positive error rates <span class="math notranslate nohighlight">\(\alpha_{lo}\)</span>
and <span class="math notranslate nohighlight">\(\alpha_{hi}\)</span>
such that <span class="math notranslate nohighlight">\(\alpha_{lo}+\alpha_{hi}=1,\)</span>
we denote by <span class="math notranslate nohighlight">\(\widehat{q}_{\alpha_{lo}}\)</span> and
<span class="math notranslate nohighlight">\(\widehat{q}_{1-\alpha_{hi}}\)</span>
the predictors of the <span class="math notranslate nohighlight">\(\alpha_{lo}\)</span> <em>-th</em> and <span class="math notranslate nohighlight">\((1-\alpha_{hi})\)</span> <em>-th</em> quantiles of <span class="math notranslate nohighlight">\(Y | X.\)</span>
The quantile predictors are trained on <span class="math notranslate nohighlight">\(D_{train}\)</span>
and calibrated on <span class="math notranslate nohighlight">\(D_{calib}\)</span>
by using the following nonconformity score:</p>
<div class="math notranslate nohighlight">
\[R_i^{} = \text{max}\{ \widehat{q}_{\alpha_{lo}}(X_i) - Y_i, Y_i - \widehat{q}_{1 - \alpha_{hi}}(X_i)\},\]</div>
<p>For example, if we set <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span>, we would fit two predictors <span class="math notranslate nohighlight">\(\widehat{q}_{0.05}(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\widehat{q}_{0.95}(\cdot)\)</span> on training data <span class="math notranslate nohighlight">\(D_{train}\)</span> and compute the scores on <span class="math notranslate nohighlight">\(D_{calibration}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is common to split evenly <span class="math notranslate nohighlight">\(\alpha\)</span> as: <span class="math notranslate nohighlight">\(\alpha_{lo} = \alpha_{hi}= \frac{\alpha}{2}\)</span>, but users are free to do otherwise.</p>
</div>
<p>The procedure, named <em>Conformalized Quantile Regression</em> <a class="reference internal" href="#romano2019" id="id6"><span>[Romano2019]</span></a>, yields the following prediction interval:</p>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new}) = \Big[ \widehat{q}_{\alpha_{lo}}(X_{new}) - \delta_{\alpha} \,,\, \widehat{q}_{1 - \alpha_{hi}}(X_{new}) + \delta_{\alpha} \Big].\]</div>
<p>When data are exchangeable, the correction margin <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> guarantees finite-sample marginal coverage for the quantile predictions, and this holds also for misspecified (i.e. “bad”) predictors.</p>
<p>If the fitted <span class="math notranslate nohighlight">\(\widehat{q}_{\alpha_{lo}}\)</span> and <span class="math notranslate nohighlight">\(\widehat{q}_{1-\alpha_{hi}}\)</span> approximate (empirically) well  the conditional distribution <span class="math notranslate nohighlight">\(Y | X\)</span> of the data, we will get a small margin <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span>: this means that on average, the prediction errors on the <span class="math notranslate nohighlight">\(D_{calib}\)</span> were small.</p>
<p>Also, if the base predictors have strong theoretical properties, our CP procedure inherits these properties of <span class="math notranslate nohighlight">\(\widehat{q}_{}(\cdot)\)</span>.
We could have an asymptotically, conditionally accurate predictor and also have a theoretically valid, distribution-free guarantee on the marginal coverage!</p>
</section>
</section>
<section id="cross-validation-cv-jackknife">
<h3>Cross-validation+ (CV+), Jackknife+<a class="headerlink" href="#cross-validation-cv-jackknife" title="Permalink to this heading"></a></h3>
<p id="theory-cvplus">The <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">leave-one-out (LOO) and the k-fold cross-validation</a> are well known schemes used to estimate regression residuals on out-of-sample data.
As shown below, one first splits the data into K partitions and then <em>holds out</em> a partition at a time to compute errors (nonconformity scores, in our case).
Following this principle, <a class="reference internal" href="#barber2021" id="id7"><span>[Barber2021]</span></a> introduced the LOO <em>jackknife+</em> (JP) and the k-fold <em>Cross-validation+</em> (CV+).
With these methods, one does <em>not need</em> a dedicated calibration set.</p>
<a class="reference internal image-reference" href="_images/k-fold-scheme.png"><img alt="_images/k-fold-scheme.png" class="align-center" src="_images/k-fold-scheme.png" style="width: 600px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The CV+ algorithm goes as follows.
Let <span class="math notranslate nohighlight">\(n = |D_{train}|\)</span>, and let <span class="math notranslate nohighlight">\(D_{train}\)</span> be partitioned disjointly into the sets <span class="math notranslate nohighlight">\(S_1, S_2, \dots, S_K\)</span>.
Each training point <span class="math notranslate nohighlight">\((X_i,Y_i) \in D_{train}\)</span> belongs to one partition, noted as <span class="math notranslate nohighlight">\(S_{k(i)}\)</span>.</p>
<p>At training, we fit and store in memory <span class="math notranslate nohighlight">\(K\)</span> models, referred to as <span class="math notranslate nohighlight">\(\widehat{f}_{-S_{K}}\)</span> to indicate that it was fitted using all data points <em>except</em> those in partition <span class="math notranslate nohighlight">\(S_{K}\)</span>.
Then, the conformalization step boils down to computing, for each <span class="math notranslate nohighlight">\((X_i,Y_i) \in D_{train}\)</span>, the score:</p>
<div class="math notranslate nohighlight">
\[R_i^{CV} = | Y_i - \widehat{f}_{-S_{k(i)}}(X_i)|, i=1, \dots, n\]</div>
<p>If <span class="math notranslate nohighlight">\(K = n\)</span>, we obtain the <em>Jackknife+</em>, <strong>leave-one-out</strong> version of the algorithm.</p>
<p><strong>Inference</strong></p>
<p>The lower and upper bounds of the prediction interval are given by:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compute <span class="math notranslate nohighlight">\(\bar{R}_{L} = \{ \widehat{f}_{-S_{k(i)}}(X_{new}) - R_i^{CV} \}_{i=1}^{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\widehat{L}_{\alpha}(X_{new}) = \lfloor \alpha (n+1) \rfloor\)</span>-th smallest value in <span class="math notranslate nohighlight">\(\bar{R}_{L}\)</span> (lower bound)</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\bar{R}_{U} = \{ \widehat{f}_{-S_{k(i)}}(X_{new}) + R_i^{CV} \}_{i=1}^{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\widehat{U}_{\alpha}(X_{new}) = \lceil (1-\alpha) (n+1) \rceil\)</span>-th smallest value in <span class="math notranslate nohighlight">\(\bar{R}_{U}\)</span> (upper bound)</p></li>
</ol>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new}) = \Big[ \widehat{L}_{\alpha}(X_{new}), \widehat{U}_{\alpha}(X_{new}) \Big].\]</div>
</section>
<section id="ensemble-batch-prediction-intervals-enbpi">
<h3>Ensemble Batch Prediction Intervals (EnbPI)<a class="headerlink" href="#ensemble-batch-prediction-intervals-enbpi" title="Permalink to this heading"></a></h3>
<p id="theory-enbpi">Introduced in <a class="reference internal" href="#xu2021" id="id8"><span>[Xu2021]</span></a>,
the EnbPI algorithm builds prediction intervals
for time series data of the form
<span class="math notranslate nohighlight">\(Y_t = f(X_t) + \epsilon_t\)</span>,
where <span class="math notranslate nohighlight">\(\epsilon_t\)</span> are identically distributed,
but not necessarily independent.
Given a training dataset <span class="math notranslate nohighlight">\(D=\lbrace (X_i, Y_i) \rbrace_{i=1}^n\)</span>
and a test set <span class="math notranslate nohighlight">\(D_{test} = \lbrace (X_t,Y_t) \rbrace_{t=n+1}^{n_{test}}\)</span>,
the EnbPI algorithm aims at constructing prediction sets
for each test point <span class="math notranslate nohighlight">\(X_t\)</span>.
As with the CV+ or Jackknife+ methods,
the EnbPI algorithm does not require a held-out calibration set,
as it uses a bootstrap algorithm instead.
Let <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be a training algorithm
(i.e. an algorithm that maps a dataset to a predictor),
and <span class="math notranslate nohighlight">\(\phi\)</span> an aggregation function
that aggregates different individual models together,
e.g. via a simple average, a bagging or an ensembling method.
The algorithm EnbPI is performed in three stages:</p>
<dl class="simple">
<dt><strong>Training</strong></dt><dd><ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(B\)</span> bootstrap datasets <span class="math notranslate nohighlight">\(S_b\)</span>, for <span class="math notranslate nohighlight">\(b=1,\dots, B\)</span> with replacement from <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
<li><p>Train <span class="math notranslate nohighlight">\(B\)</span> bootstrap models <span class="math notranslate nohighlight">\(\widehat{f}^b = \mathcal{A}(S_b)\)</span>.</p></li>
</ol>
</dd>
<dt><strong>Calibration</strong></dt><dd><ol class="arabic simple">
<li><p>Compute the predictions on each training sample <span class="math notranslate nohighlight">\(X_i\in D\)</span>. Only the models <span class="math notranslate nohighlight">\(\widehat{f}^b\)</span> where <span class="math notranslate nohighlight">\(X_i\not\in S_b\)</span> are used in the aggregation: <span class="math notranslate nohighlight">\(\widehat{f}_{-i}(X_i):=\phi\big( \lbrace \widehat{f}^b(X_i) | X_i\not\in S_b\rbrace\big)\)</span>.</p></li>
<li><p>Compute the errors <span class="math notranslate nohighlight">\(R_i=|Y_i-\widehat{f}_{-i}(X_i)|\)</span>, and store them as <span class="math notranslate nohighlight">\(\mathcal{R}_1:=\lbrace R_i,i=1,\dots, n\rbrace\)</span>.</p></li>
</ol>
</dd>
<dt><strong>Inference</strong></dt><dd><ol class="arabic simple">
<li><p>Compute the predictions on each test sample <span class="math notranslate nohighlight">\(X_t\in D_{test}\)</span> by setting <span class="math notranslate nohighlight">\(\widehat{f}_{-t}(X_t):=  \frac{1}{T}\sum_{i=1}^T \widehat{f}_{-i}(X_t)\)</span>.</p></li>
<li><p>Update the error set: <span class="math notranslate nohighlight">\(\mathcal R_t\)</span> (see below).</p></li>
<li><p>Compute the width of the prediction intervals <span class="math notranslate nohighlight">\(\delta_{\alpha, t}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}_t\)</span>.</p></li>
</ol>
</dd>
</dl>
<p>The prediction interval for <span class="math notranslate nohighlight">\(X_t\)</span> is then given by</p>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha} = \big[ \widehat{f}_{-t}(X_t)-\delta_{\alpha, t}, \widehat{f}_{-t}(X_t)+\delta_{\alpha, t}].\]</div>
<p>In order to update the error set <span class="math notranslate nohighlight">\(\mathcal{R}_t\)</span>,
a <em>memory</em> parameter <span class="math notranslate nohighlight">\(s\)</span> is employed.
Every <span class="math notranslate nohighlight">\(s\)</span> test examples, the first <span class="math notranslate nohighlight">\(s\)</span> errors in the set
<span class="math notranslate nohighlight">\(\mathcal{R}\)</span> are dropped and the errors over the last <span class="math notranslate nohighlight">\(s\)</span>
test examples are added to the error set <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.
I.e. if <span class="math notranslate nohighlight">\(t-n = 0\ mod\ s\)</span> then <span class="math notranslate nohighlight">\(\mathcal{R}_t = \lbrace R_i, i=t-n,\dots,t-1\rbrace\)</span>
and if <span class="math notranslate nohighlight">\(t-n \neq 0\ mod\ s\)</span> then <span class="math notranslate nohighlight">\(\mathcal{R}_t=\mathcal{R}_{t-1}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The EnbPI algorithm does not provide an exact probabilistic guarantee as the previous CP methods do.
The guarantee provided by the EnbPI algorithm is only approximate,
and holds under additional assumptions on the error process
<span class="math notranslate nohighlight">\(\epsilon_t\)</span>. However, it does not require the data to be exchangeable.</p>
</div>
</section>
</section>
<section id="conformal-classification">
<h2>Conformal Classification<a class="headerlink" href="#conformal-classification" title="Permalink to this heading"></a></h2>
<section id="least-ambiguous-set-valued-classifiers-lac">
<h3>Least Ambiguous Set-Valued Classifiers (LAC)<a class="headerlink" href="#least-ambiguous-set-valued-classifiers-lac" title="Permalink to this heading"></a></h3>
<p id="theory-lac">As for the Split Conformal Regression algorithm,
the LAC algorithm introduced in <a class="reference internal" href="#sadinle2018" id="id9"><span>[Sadinle2018]</span></a>
requires us to split the dataset <span class="math notranslate nohighlight">\(D\)</span> into a proper training set <span class="math notranslate nohighlight">\(D_{train}\)</span>
and an independent calibration set <span class="math notranslate nohighlight">\(D_{calib}\)</span>.
A classifier <span class="math notranslate nohighlight">\(\widehat{\pi}\)</span> is trained
using the proper training set <span class="math notranslate nohighlight">\(D_{train}\)</span> only.
We assume that the output of the classifier is given by the softmax scores for the different classes.
I.e. for each input <span class="math notranslate nohighlight">\(x\)</span>,
the output <span class="math notranslate nohighlight">\(\widehat{\pi}(x)=(\widehat{\pi}_1(x),\dots,\widehat{\pi}_K(x))\)</span>
is a probability vector and <span class="math notranslate nohighlight">\(k=1,\dots, K\)</span>
represent the possible different classes in the classification task.</p>
<p>In order to construct the prediction sets <span class="math notranslate nohighlight">\(\widehat{C}_\alpha\)</span>,
the LAC algorithm works in two stages:</p>
<dl>
<dt><strong>Calibration</strong></dt><dd><ol class="arabic simple">
<li><p>For each example <span class="math notranslate nohighlight">\(X_i\)</span> in the calibration dataset, compute the error <span class="math notranslate nohighlight">\(R_i=1-\widehat{\pi}_{Y_i}(X_i)\)</span>, i.e. 1 minus the sofmax output of the ground truth class.</p></li>
<li><p>store all errors in a vector <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
</ol>
</dd>
<dt><strong>Inference</strong></dt><dd><ol class="arabic simple">
<li><p>Compute the probability threshold <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)(1 + 1/n_{calib})\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
<li><p>The prediction set for a test point <span class="math notranslate nohighlight">\(X_{new}\)</span> is defined as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new})=\big\lbrace
k \, | \, \widehat{\pi}_{k}(X_{new})\geq 1 - \delta_\alpha
\big\rbrace\,.\]</div>
</dd>
</dl>
</section>
<section id="adaptive-prediction-sets-aps">
<h3>Adaptive Prediction Sets (APS)<a class="headerlink" href="#adaptive-prediction-sets-aps" title="Permalink to this heading"></a></h3>
<p id="theory-aps">The LAC algorithm produces prediction sets that have small average size, and is known to be Bayes optimal.
However, it tends to undercover in regions where the classifier is uncertain, and overcover in regions where the classifier is confident.
The APS algorithm introduced in <a class="reference internal" href="#romano2020" id="id10"><span>[Romano2020]</span></a> aims to produce prediction sets that are more stable and have a better coverage rate.
We represent by <span class="math notranslate nohighlight">\(\widehat{\pi}_{(1)}(x)\geq \cdots\geq \widehat{\pi}_{(K)}(x)\)</span>
the softmax vector <span class="math notranslate nohighlight">\(\widehat{\pi}\)</span> arranged in decreasing order,
i.e. <span class="math notranslate nohighlight">\((k)\)</span> is the index of the class having the <span class="math notranslate nohighlight">\(k\)</span>-th largest probability mass.</p>
<p>In order to construct the prediction sets <span class="math notranslate nohighlight">\(\widehat{C}_\alpha\)</span>,
the APS algorithm works in two stages:</p>
<dl>
<dt><strong>Calibration</strong></dt><dd><ol class="arabic simple">
<li><p>For each example <span class="math notranslate nohighlight">\(X_i\)</span> in the calibration dataset, we compute the error <span class="math notranslate nohighlight">\(R_i\)</span> as the probability mass needed for reaching the true label <span class="math notranslate nohighlight">\(Y_i\)</span>, i.e. <span class="math notranslate nohighlight">\(R_i=\widehat{\pi}_{(1)}+\cdots+\widehat{\pi}_{(k)}\)</span>, wehere <span class="math notranslate nohighlight">\((k)=Y_i\)</span>.</p></li>
<li><p>Store all errors in a vector <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
</ol>
</dd>
<dt><strong>Inference</strong></dt><dd><ol class="arabic simple">
<li><p>Compute the probability threshold <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)(1 + 1/n_{calib})\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
<li><p>The prediction set for a test point <span class="math notranslate nohighlight">\(X_{new}\)</span> is defined as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\widehat{C}_{\alpha}(X_{new})=\big\lbrace
(1),\dots,(k)
\big\rbrace\quad \text{where}\quad
k = \min\big\lbrace i : \widehat{\pi}_{(1)}+\cdots+\widehat{\pi}_{(i)}\geq \delta_\alpha\big\rbrace.\]</div>
</dd>
</dl>
</section>
<section id="regularized-adaptive-prediction-sets-raps">
<h3>Regularized Adaptive Prediction Sets (RAPS)<a class="headerlink" href="#regularized-adaptive-prediction-sets-raps" title="Permalink to this heading"></a></h3>
<p id="theory-raps">The RAPS algorithm introduced in <a class="reference internal" href="#angelopoulos2021" id="id11"><span>[Angelopoulos2021]</span></a> is a modification of the APS algorithm
that uses a regularization term in order to produce smaller and more stable prediction sets.
Employing the same notations as for the APS algorithm above,
the RAPS algorithm works in two stages:</p>
<dl>
<dt><strong>Calibration</strong></dt><dd><ol class="arabic">
<li><p>For each example <span class="math notranslate nohighlight">\(X_i\)</span> in the calibration dataset, we compute the error <span class="math notranslate nohighlight">\(R_i\)</span> as the probability mass needed for reaching the true label <span class="math notranslate nohighlight">\(Y_i\)</span>, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[R_i=\widehat{\pi}_{(1)}+\cdots+\widehat{\pi}_{(k)} + \lambda(k-k_{reg}+1),\]</div>
<p>where <span class="math notranslate nohighlight">\((k)=Y_i\)</span>. The regularization term <span class="math notranslate nohighlight">\(\lambda(k-k_{reg}+1)\)</span> is added to the APS error, where <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(k_{reg}\)</span> are hyperparameters.</p>
</div></blockquote>
</li>
<li><p>Store all errors in a vector <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
</ol>
</dd>
<dt><strong>Inference</strong></dt><dd><ol class="arabic">
<li><p>Compute the probability thresholdn <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)(1 + 1/n_{calib})\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
<li><p>The prediction set for a test point <span class="math notranslate nohighlight">\(X_{new}\)</span> is defined as <span class="math notranslate nohighlight">\(\widehat{C}_{\alpha}(X_{new})=\big\lbrace (1),\dots,(k)\big\rbrace\)</span>, where</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[k = \max\big\lbrace i : \widehat{\pi}_{(1)}+\cdots+\widehat{\pi}_{(i)} + \lambda(i-k_{reg}+1) \leq \delta_\alpha\big\rbrace + 1.\]</div>
</div></blockquote>
</li>
</ol>
</dd>
</dl>
</section>
</section>
<section id="conformal-anomaly-detection">
<h2>Conformal Anomaly Detection<a class="headerlink" href="#conformal-anomaly-detection" title="Permalink to this heading"></a></h2>
<p id="theory-cad">Conformal prediction can be extended to handle unsupervised anomaly detection, allowing us to identify data points that do not conform to the “normal” (or nominal) distribution of a dataset <a class="reference internal" href="#laxhammar2015" id="id12"><span>[Laxhammar2015]</span></a>. The goal is to assign a statistical guarantee to the anomaly detector, ensuring that it controls the <strong>false positive rate</strong>.</p>
<p>To detect anomalies, we start with a model that assigns an anomaly score <span class="math notranslate nohighlight">\(s(X)\)</span> to each data point. Higher scores indicate a higher likelihood of being an outlier.</p>
<p><strong>Calibration</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>For each example <span class="math notranslate nohighlight">\(X_i\)</span> in the calibration dataset, we compute the nonconformity score as the anomaly score provided by the model, i.e. <span class="math notranslate nohighlight">\(R_i = s(X_i)\)</span>.</p></li>
<li><p>Store all nonconformity scores in a vector <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
</ol>
</div></blockquote>
<p><strong>Inference</strong></p>
<blockquote>
<div><ol class="arabic">
<li><p>Compute the anomaly score threshold <span class="math notranslate nohighlight">\(\delta_{\alpha}\)</span> as the <span class="math notranslate nohighlight">\((1-\alpha)(1 + 1/n_{calib})\)</span>-th empirical quantile of <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>.</p></li>
<li><p>For a new test point <span class="math notranslate nohighlight">\(X_{new}\)</span>, the conformalized anomaly detector classifies it as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\widehat{C}_{\alpha} = \begin{cases}
\text{Normal} &amp; \text{if } s(X_{new}) \leq \delta_{\alpha} \\
\text{Anomaly} &amp; \text{if } s(X_{new}) &gt; \delta_{\alpha}
\end{cases}\end{split}\]</div>
</div></blockquote>
</li>
</ol>
</div></blockquote>
<p>Conformal anomaly detection provides an error control guarantee, meaning that under the assumption of exchangeability, the probability of a false positive (labeling a norminal instance as an anomaly) is bounded by <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</section>
<section id="conformal-object-detection">
<h2>Conformal Object Detection<a class="headerlink" href="#conformal-object-detection" title="Permalink to this heading"></a></h2>
<p id="theory-splitboxwise">Source: <a class="reference internal" href="#degrancey2022" id="id13"><span>[deGrancey2022]</span></a></p>
<p>TBC</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading"></a></h2>
<div role="list" class="citation-list">
<div class="citation" id="angelopoulos2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Angelopoulos2021</a><span class="fn-bracket">]</span></span>
<p>Angelopoulos, A. N., Bates, S., Jordan, M., &amp; Malik, J (2021). Uncertainty Sets for Image Classifiers using Conformal Prediction. In Proceedings of ICLR 2021. <a class="reference external" href="https://openreview.net/forum?id=eNdiU_DbM9">https://openreview.net/forum?id=eNdiU_DbM9</a></p>
</div>
<div class="citation" id="angelopoulos2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Angelopoulos2022</a><span class="fn-bracket">]</span></span>
<p>Angelopoulos, A.N. and Bates, S., (2021). A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511. https://arxiv.org/abs/2107.07511</p>
</div>
<div class="citation" id="barber2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Barber2021</a><span class="fn-bracket">]</span></span>
<p>Barber, R. F., Candes, E. J., Ramdas, A., &amp; Tibshirani, R. J. (2021). Predictive inference with the jackknife+. Ann. Statist. 49 (1) 486 - 507, February 2021. <a class="reference external" href="https://arxiv.org/abs/1905.02928">https://arxiv.org/abs/1905.02928</a></p>
</div>
<div class="citation" id="laxhammar2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">Laxhammar2015</a><span class="fn-bracket">]</span></span>
<p>Laxhammar, R., &amp; Falkman, G. (2015). Inductive conformal anomaly detection for sequential detection of anomalous sub-trajectories. Annals of Mathematics and Artificial Intelligence, 74, 67-94</p>
</div>
<div class="citation" id="lei2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Lei2018</a><span class="fn-bracket">]</span></span>
<p>Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R.J. and Wasserman, L., (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), pp.1094-1111. <a class="reference external" href="https://arxiv.org/abs/1604.04173">https://arxiv.org/abs/1604.04173</a></p>
</div>
<div class="citation" id="papadopoulos2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Papadopoulos2002</a><span class="fn-bracket">]</span></span>
<p>Papadopoulos, H., Proedrou, K., Vovk, V. and Gammerman, A., (2002). Inductive confidence machines for regression. In Proceedings of ECML 2002, Springer. <a class="reference external" href="https://link.springer.com/chapter/10.1007/3-540-36755-1_29">https://link.springer.com/chapter/10.1007/3-540-36755-1_29</a></p>
</div>
<div class="citation" id="papadopoulos2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Papadopoulos2008</a><span class="fn-bracket">]</span></span>
<p>Papadopoulos, H., Gammerman, A. and Vovk, V., (2008). Normalized nonconformity measures for regression conformal prediction. In Proceedings of the IASTED International Conference on Artificial Intelligence and Applications (AIA 2008) (pp. 64-69).</p>
</div>
<div class="citation" id="degrancey2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">deGrancey2022</a><span class="fn-bracket">]</span></span>
<p>de Grancey, F., Adam, J.L., Alecu, L., Gerchinovitz, S., Mamalet, F. and Vigouroux, D., 2022, June. Object detection with probabilistic guarantees: A conformal prediction approach. In International Conference on Computer Safety, Reliability, and Security.</p>
</div>
<div class="citation" id="romano2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Romano2019</a><span class="fn-bracket">]</span></span>
<p>Romano, Y., Patterson, E. and Candes, E., (2019). Conformalized quantile regression. In Proceedings of NeurIPS, 32. <a class="reference external" href="https://arxiv.org/abs/1905.03222">https://arxiv.org/abs/1905.03222</a></p>
</div>
<div class="citation" id="romano2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Romano2020</a><span class="fn-bracket">]</span></span>
<p>Romano, Y., Sesia, M., &amp; Candes, E. (2020). Classification with valid and adaptive coverage. In Proceedings of NeurIPS, 33. <a class="reference external" href="https://arxiv.org/abs/2006.02544">https://arxiv.org/abs/2006.02544</a></p>
</div>
<div class="citation" id="sadinle2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Sadinle2018</a><span class="fn-bracket">]</span></span>
<p>Sandinle, M., Lei, J., Wasserman, L., (2018). Least Ambiguous Set-Valued Classifiers With Bounded Error Levels. Journal of the American Statistical Association, 114(525), 223-234. <a class="reference external" href="https://arxiv.org/abs/1609.00451">https://arxiv.org/abs/1609.00451</a></p>
</div>
<div class="citation" id="xu2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Xu2021</a><span class="fn-bracket">]</span></span>
<p>Xu, C. &amp; Xie, Y.. (2021). Conformal prediction interval for dynamic time-series. Proceedings of ICML 2021. <a class="reference external" href="https://proceedings.mlr.press/v139/xu21h.html">https://proceedings.mlr.press/v139/xu21h.html</a>.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plotting.html" class="btn btn-neutral float-left" title="🖼️ Plotting" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IRT Antoine de Saint Exupéry - All rights reserved. DEEL is a research program operated by IVADO, IRT Saint Exupéry, CRIAQ and ANITI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>