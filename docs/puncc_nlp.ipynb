{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254914e",
   "metadata": {
    "id": "f254914e"
   },
   "source": [
    "# üìò Tutorial: Conformal Prediction for Text Classification with PUNCC\n",
    "\n",
    "In this tutorial, you will discover how to use the **PUNCC** library for uncertainty quantification on an NLP classification task. The example uses the **AG News** dataset and the **DistilBERT** model.\n",
    "\n",
    "By the end of this notebook, you will be able to transform your own NLP classification models into conformal predictors and evaluate their performance effectively.\n",
    "\n",
    "‚ö° If you are only interested in the [üìè conformal text classification](#cr-conformal) section, you can execute all cells up to that point and skip the details about data loading, preprocessing and model training.\n",
    "\n",
    "-------\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "- [‚öôÔ∏è Setup](#cr-setup)\n",
    "- [üìö Dataset, Model, Tokenizer](#cr-data)\n",
    "- [üßº Preprocessing](#cr-preprocessing)\n",
    "- [üöÄ Training](#cr-training)\n",
    "- [üìè Conformal Text Classification](#cr-conformal)\n",
    "\n",
    "**Links**\n",
    "- [<img src=\"https://github.githubassets.com/images/icons/emoji/octocat.png\" width=20> Github](https://github.com/deel-ai/puncc)\n",
    "- [üìò Documentation](https://deel-ai.github.io/puncc/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d532e",
   "metadata": {
    "id": "8f3d532e"
   },
   "source": [
    "# ‚öôÔ∏è Setup: Install and Import Libraries <a class=\"anchor\" id=\"cr-setup\"></a>\n",
    "In addition to **PUNCC**, we will be using the following libraries from HuggingFace:\n",
    "- **transformers**: Provides pre-trained NLP models and training utilities.\n",
    "- **datasets**: For downloading benchmark datasets like AG News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WPMbPPoit5cL",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: puncc in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from puncc) (1.5.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from puncc) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from puncc) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from puncc) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from puncc) (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from puncc) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->puncc) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->puncc) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->puncc) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->puncc) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->puncc) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->puncc) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install puncc datasets transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fUER5enM5A4-",
   "metadata": {
    "id": "fUER5enM5A4-"
   },
   "source": [
    "We import the general-purpose modules that will be used throughout the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d98bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d3be4",
   "metadata": {
    "id": "320d3be4"
   },
   "source": [
    "# üìö Dataset, Model, Tokenizer <a class=\"anchor\" id=\"cr-data\"></a>\n",
    "We load the **AG News** dataset, a 4-class dataset containing short news articles categorized as World, Sports, Business, Sci/Tech.\n",
    "We also load a pretrained **DistilBERT** base model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a644b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f21e2",
   "metadata": {
    "id": "192f21e2"
   },
   "source": [
    "# üßº Preprocess the Data  <a class=\"anchor\" id=\"cr-preprocessing\"></a>\n",
    "We split the training dataset into a proper training set and a calibration set, which will be use to conformalize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f69e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset: 20k train, 5k calibration\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=5_000, train_size=20_000, seed=42)\n",
    "\n",
    "# Rename keys\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"calib\": split_dataset[\"test\"],\n",
    "    \"test\": dataset[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56945e",
   "metadata": {
    "id": "ff56945e"
   },
   "source": [
    "The following preprocessing function operates as follows:\n",
    "- Tokenizes text with padding/truncation.\n",
    "- Renames and reformats the dataset for PyTorch/Trainer use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48c759aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25025dcbbf4d47c899a326d66d54c799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded_dataset = split_dataset.map(preprocess, batched=True)\n",
    "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307993b8",
   "metadata": {
    "id": "307993b8"
   },
   "source": [
    "# üöÄ Training <a class=\"anchor\" id=\"cr-training\"></a>\n",
    "We set up the training loop using Hugging Face's high-level `Trainer` API, and we train for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "z_Llv50D-zlH",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 04:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.246922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.3050960479736328, metrics={'train_runtime': 272.3826, 'train_samples_per_second': 73.426, 'train_steps_per_second': 4.589, 'total_flos': 662360616960000.0, 'train_loss': 0.3050960479736328, 'epoch': 1.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./news_results\",\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374449de",
   "metadata": {
    "id": "374449de"
   },
   "source": [
    "Next we evaluate the model using traditional accuracy and classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8077435b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9181578947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92      1900\n",
      "           1       0.96      0.99      0.97      1900\n",
      "           2       0.87      0.89      0.88      1900\n",
      "           3       0.89      0.91      0.90      1900\n",
      "\n",
      "    accuracy                           0.92      7600\n",
      "   macro avg       0.92      0.92      0.92      7600\n",
      "weighted avg       0.92      0.92      0.92      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_output = trainer.predict(encoded_dataset[\"test\"])\n",
    "preds = preds_output.predictions.argmax(axis=1)\n",
    "labels = preds_output.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(labels, preds))\n",
    "print(classification_report(labels, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc1266",
   "metadata": {
    "id": "67cc1266"
   },
   "source": [
    "We Apply the softmax to the raw logits to convert them to probabilities, These softmax scores are used for conformal calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a0316ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calib_preds = trainer.predict(encoded_dataset[\"calib\"])\n",
    "calib_logits, calib_labels = calib_preds.predictions, calib_preds.label_ids\n",
    "calib_softmax = F.softmax(torch.tensor(calib_logits), dim=1).numpy()\n",
    "\n",
    "test_preds = trainer.predict(encoded_dataset[\"test\"])\n",
    "test_logits, test_labels = test_preds.predictions, test_preds.label_ids\n",
    "test_softmax = F.softmax(torch.tensor(test_logits), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e4763",
   "metadata": {
    "id": "9c0e4763"
   },
   "source": [
    "# üìè 9. Conformal Text Classification with PUNCC <a class=\"anchor\" id=\"cr-conformal\"></a>\n",
    "We will conformalize our model using the  RAPS algorithm. We proceed as follows:\n",
    "- **Instantiate the `IdPredictor` dummy wrapper**. The model is already trained, so we will not be training it using PUNCC, therefore we instantiate the `IdPredictor` dummy wrapper.\n",
    "- **Instantiate the `LAC` conformalizer**. We instantiate the `RAPS` conformalizer with the dummy predictor.\n",
    "- **Fit the conformal predictor.** `fit()` calibrates the conformal predictor, i.e., computes the nonconformity scores proper to the `RAPS` conformal method.\n",
    "- **Predict**. `predict()` returns both the original (softmax) model predictions as well as the **prediction sets** obtained with the `RAPS` conformal method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9305d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deel.puncc.api.prediction import IdPredictor\n",
    "from deel.puncc.classification import LAC\n",
    "\n",
    "dummy_predictor = IdPredictor()\n",
    "lac_cp = LAC(dummy_predictor, train=False)\n",
    "lac_cp.fit(X_calib=calib_softmax, y_calib=calib_labels)\n",
    "\n",
    "y_pred, y_pred_set = lac_cp.predict(X_test=test_softmax, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debc738",
   "metadata": {
    "id": "9debc738"
   },
   "source": [
    "We finally compute the common conformal metrics on the test set: the average coverage of the prediction sets and the average cardinality of the prediction sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1096d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical coverage : 0.992\n",
      "Average set size : 1.581\n"
     ]
    }
   ],
   "source": [
    "from deel.puncc import metrics\n",
    "\n",
    "mean_coverage = metrics.classification_mean_coverage(test_labels, y_pred_set)\n",
    "mean_size = metrics.classification_mean_size(y_pred_set)\n",
    "\n",
    "print(f\"Empirical coverage : {mean_coverage:.3f}\")\n",
    "print(f\"Average set size : {mean_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGJWK3qN_SjB",
   "metadata": {
    "id": "YGJWK3qN_SjB"
   },
   "source": [
    "Let us print an example of a test sample along with the model's point prediction and the conformalized prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "_nBvvQ29_ewC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text from test set:\n",
      "Scientists Discover Ganymede has a Lumpy Interior Jet Propulsion Lab -- Scientists have discovered irregular lumps beneath the icy surface of Jupiter's largest moon, Ganymede. These irregular masses may be rock formations, supported by Ganymede's icy shell for billions of years...\n",
      "Point prediction of the model:\n",
      "Sci/Tech\n",
      "Conformal prediction set:\n",
      "['World', 'Sci/Tech']\n",
      "True label:\n",
      "Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample index from the test set\n",
    "idx = 16\n",
    "\n",
    "# Extract the sample text\n",
    "sample_text = split_dataset[\"test\"][idx]['text']  # or adjust key as per your dataset\n",
    "\n",
    "# Extract point prediction\n",
    "point_label = test_labels[idx]\n",
    "\n",
    "# Extract conformal prediction set\n",
    "conformal_labels = y_pred_set[idx]\n",
    "\n",
    "# Map label indexes to class labels\n",
    "label_mapping = dataset['train'].features['label'].names\n",
    "point_label = label_mapping[point_label]\n",
    "conformal_labels = [label_mapping[label] for label in conformal_labels]\n",
    "\n",
    "# Print results\n",
    "print(\"Sample text from test set:\")\n",
    "print(sample_text)\n",
    "print(\"Point prediction of the model:\")\n",
    "print(point_label)\n",
    "print(\"Conformal prediction set:\")\n",
    "print(conformal_labels)\n",
    "print(\"True label:\")\n",
    "print(label_mapping[test_labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1xK2V4uMDNDb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
